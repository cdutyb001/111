/*
 * Gaussian-LIC: Real-Time Photo-Realistic SLAM with Gaussian Splatting and LiDAR-Inertial-Camera Fusion
 * Copyright (C) 2025 Xiaolei Lang
 *
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program.  If not, see <https://www.gnu.org/licenses/>.
 */

#pragma once

#include <vector>

#include <torch/torch.h>

#include <fused-ssim/ssim.h>

namespace loss_utils
{

inline torch::Tensor l1_loss(torch::Tensor &network_output, torch::Tensor &gt)
{
    return torch::abs(network_output - gt).mean();
}

inline torch::Tensor psnr(torch::Tensor &img1, torch::Tensor &img2)
{
    auto mse = torch::pow(img1 - img2, 2).mean();
    return 10.0f * torch::log10(1.0f / mse);
}

/** def psnr(img1, img2):
 *     mse = (((img1 - img2)) ** 2).view(img1.shape[0], -1).mean(1, keepdim=True)
 *     return 20 * torch.log10(1.0 / torch.sqrt(mse))
 */
inline torch::Tensor psnr_gaussian_splatting(torch::Tensor &img1, torch::Tensor &img2)
{
    auto mse = torch::pow(img1 - img2, 2).view({img1.size(0) , -1}).mean(1, /*keepdim=*/true);
    return 20.0f * torch::log10(1.0f / torch::sqrt(mse)).mean();
}

inline torch::Tensor gaussian(
    int window_size,
    float sigma,
    torch::DeviceType device_type = torch::kCUDA)
{
    std::vector<float> gauss_values(window_size);
    for (int x = 0; x < window_size; ++x) {
        int temp = x - window_size / 2;
        gauss_values[x] = std::exp(-temp * temp / (2.0f * sigma * sigma));
    }
    torch::Tensor gauss = torch::tensor(
        gauss_values,
        torch::TensorOptions().device(device_type));
    return gauss / gauss.sum();
}

inline torch::autograd::Variable create_window(
    int window_size,
    int64_t channel,
    torch::DeviceType device_type = torch::kCUDA)
{
    auto _1D_window = gaussian(window_size, 1.5f, device_type).unsqueeze(1);
    auto _2D_window = _1D_window.mm(_1D_window.t()).to(torch::kFloat).unsqueeze(0).unsqueeze(0);
    auto window = torch::autograd::Variable(_2D_window.expand({channel, 1, window_size, window_size}).contiguous());
    return window;
}

/*ssim*/

inline torch::Tensor _ssim(
    torch::Tensor &img1,
    torch::Tensor &img2,
    torch::autograd::Variable &window,
    int window_size,
    int64_t channel,
    bool size_average = true)
{
    int window_size_half = window_size / 2;
    auto mu1 = torch::nn::functional::conv2d(img1, window, torch::nn::functional::Conv2dFuncOptions().padding(window_size_half).groups(channel));
    auto mu2 = torch::nn::functional::conv2d(img2, window, torch::nn::functional::Conv2dFuncOptions().padding(window_size_half).groups(channel));

    auto mu1_sq = mu1.pow(2);
    auto mu2_sq = mu2.pow(2);
    auto mu1_mu2 = mu1 * mu2;

    auto sigma1_sq = torch::nn::functional::conv2d(img1 * img1, window, torch::nn::functional::Conv2dFuncOptions().padding(window_size_half).groups(channel))
                    - mu1_sq;
    auto sigma2_sq = torch::nn::functional::conv2d(img2 * img2, window, torch::nn::functional::Conv2dFuncOptions().padding(window_size_half).groups(channel))
                    - mu2_sq;
    auto sigma12 = torch::nn::functional::conv2d(img1 * img2, window, torch::nn::functional::Conv2dFuncOptions().padding(window_size_half).groups(channel))
                    - mu1_mu2;

    auto C1 = 0.01 * 0.01;
    auto C2 = 0.03 * 0.03;

    auto ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2));

    if (size_average)
        return ssim_map.mean();
    else
        return ssim_map.mean(1).mean(1).mean(1);
}

inline torch::Tensor ssim(
    torch::Tensor &img1,
    torch::Tensor &img2,
    torch::DeviceType device_type = torch::kCUDA,
    int window_size = 11,
    bool size_average = true)
{
    auto channel = img1.size(-3);
    auto window = create_window(window_size, channel, device_type);

    // window = window.to(img1.device());
    window = window.type_as(img1);

    return _ssim(img1, img2, window, window_size, channel, size_average);
}

const float C1 = std::pow(0.01, 2);
const float C2 = std::pow(0.03, 2);

/*fused-ssim*/

class FusedSSIMMap : public torch::autograd::Function<FusedSSIMMap> 
{
public:
    static torch::Tensor forward(torch::autograd::AutogradContext *ctx, float C1, float C2, 
                                 torch::Tensor& img1, torch::Tensor& img2) 
    {
        std::string padding = "same";
        bool train = true;

        auto result = fusedssim(C1, C2, img1, img2, train);
        torch::Tensor ssim_map = std::get<0>(result);
        torch::Tensor dm_dmu1 = std::get<1>(result);
        torch::Tensor dm_dsigma1_sq = std::get<2>(result);
        torch::Tensor dm_dsigma12 = std::get<3>(result);

        if (padding == "valid") 
        {
            ssim_map = ssim_map.slice(2, 5, -5).slice(3, 5, -5);
        }

        ctx->save_for_backward({img1.detach(), img2, dm_dmu1, dm_dsigma1_sq, dm_dsigma12});
        ctx->saved_data["C1"] = C1;
        ctx->saved_data["C2"] = C2;
        ctx->saved_data["padding"] = padding;

        return ssim_map;
    }

    static torch::autograd::variable_list backward(torch::autograd::AutogradContext *ctx, torch::autograd::variable_list grad_output) 
    {
        auto saved = ctx->get_saved_variables();
        torch::Tensor img1 = saved[0];
        torch::Tensor img2 = saved[1];
        torch::Tensor dm_dmu1 = saved[2];
        torch::Tensor dm_dsigma1_sq = saved[3];
        torch::Tensor dm_dsigma12 = saved[4];

        float C1 = static_cast<float>(ctx->saved_data["C1"].toDouble());
        float C2 = static_cast<float>(ctx->saved_data["C2"].toDouble());
        std::string padding = ctx->saved_data["padding"].toStringRef();

        torch::Tensor dL_dmap = grad_output[0];
        if (padding == "valid") 
        {
            dL_dmap = torch::zeros_like(img1);
            dL_dmap.slice(2, 5, -5).slice(3, 5, -5) = grad_output[0];
        }

        torch::Tensor grad = fusedssim_backward(C1, C2, img1, img2, dL_dmap, dm_dmu1, dm_dsigma1_sq, dm_dsigma12);

        return {torch::Tensor(), torch::Tensor(), grad, torch::Tensor(), torch::Tensor(), torch::Tensor()};
    }
};

inline torch::Tensor fused_ssim(torch::Tensor& img1, torch::Tensor& img2) 
{    
    torch::Tensor map = FusedSSIMMap::apply(C1, C2, img1, img2);
    return map.mean();
}

// ================Probabilistic Gaussian-LIC===================== â†“
/**
     * @brief ä¸ç¡®å®šæ€§åŠ æƒæ·±åº¦æŸå¤±
     * L_depth = mean((1/(U+Îµ)) * |D_render - D_prior|)
     */
inline torch::Tensor uncertainty_weighted_depth_loss(
    torch::Tensor& rendered_depth,          // æ¸²æŸ“æ·±åº¦ï¼ˆHï¼ŒWï¼‰
    torch::Tensor& prior_depth,             // å…ˆéªŒæ·±åº¦ï¼ˆHï¼ŒWï¼‰ from SGSnet
    torch::Tensor& uncertainty_map,         // ä¸ç¡®å®šæ€§å›¾(H,W)
    float epsilon = 1e-6f
)
{
    // è®¡ç®—æƒé‡ï¼šä¸ç¡®å®šæ€§è¶Šä½ï¼Œæƒé‡è¶Šé«˜
    auto weight = 1.0f/(uncertainty_map + epsilon);

    // L1æ·±åº¦å·®å¼‚
    // ä¿®æ”¹å‰1 ---ç»å¯¹å€¼å·®å¼‚
    // auto depth_diff = torch::abs(rendered_depth - prior_depth);  
    // ä¿®æ”¹å2 --- ç›¸å¯¹å€¼å·®å¼‚
    // auto depth_diff = torch::abs(rendered_depth - prior_depth) / (prior_depth + epsilon);
    // ä¿®æ”¹å3
    // auto depth_diff = torch::abs(rendered_depth - prior_depth) / prior_depth.clamp_min(1.0f);
    // ä¿®æ”¹å4
    auto depth_diff = torch::abs(rendered_depth - prior_depth) / prior_depth.clamp_min(1.0f);

    // åŠ æƒæŸå¤±
    auto weighted_loss = depth_diff * weight;

    // åªè®¡ç®—æœ‰æ•ˆåŒºåŸŸï¼ˆprior_deprh > 0ï¼‰
    auto valid_mask = (prior_depth > 0.1f) & (rendered_depth > 0.1f);
    auto masked_loss = weighted_loss * valid_mask.to(torch::kFloat32);

    // å½’ä¸€åŒ–
    auto num_valid = valid_mask.sum() + epsilon;
    return masked_loss.sum()/num_valid;
}
/**
     * @brief ä»æ·±åº¦å›¾è®¡ç®—æ³•å‘å›¾
     * ä½¿ç”¨Sobelç®—å­è®¡ç®—æ¢¯åº¦
     */
inline torch::Tensor depth_to_normal(
    torch::Tensor& depth,   //(H,W)
    float fx,float fy
)
{
    // Sobelæ ¸
    auto sobel_x = torch::tensor({{-1.0f,0.0f,1.0f},
                                {-2.0f,0.0f,2.0f},
                                {-1.0f,0.0f,1.0f}}).cuda().unsqueeze(0).unsqueeze(0);
    auto sobel_y = torch::tensor({{-1.0f,-2.0f,-1.0f},
                                {0.0f,0.0f,0.0f},
                                {1.0f,2.0f,1.0f}}).cuda().unsqueeze(0).unsqueeze(0);
    
    // æ·»åŠ batchå’Œchannelç»´åº¦
    auto depth_4d = depth.unsqueeze(0).unsqueeze(0);

    // è®¡ç®—æ¢¯åº¦
    auto grad_x = torch::nn::functional::conv2d(
        depth_4d,sobel_x,
        torch::nn::functional::Conv2dFuncOptions().padding(1)
    );
    auto grad_y = torch::nn::functional::conv2d(
        depth_4d,sobel_y,
        torch::nn::functional::Conv2dFuncOptions().padding(1)
    );

    //è½¬æ¢ä¸º3Dæ³•å‘
    grad_x = grad_x.squeeze()/fx;// dz/dx
    grad_y = grad_y.squeeze()/fy;// dz/dy

    // æ³•å‘é‡ n = ï¼ˆ-dz/dxï¼Œdz/dyï¼Œ1ï¼‰ ç„¶åå½’ä¸€ä¸‹
    auto normal = torch::stack({-grad_x, -grad_y, torch::ones_like(grad_x)}, 0);
    auto normal_norm = torch::norm(normal,2,0,true) + 1e-6F;
    normal = normal / normal_norm;

    return normal; // ï¼ˆ3ï¼ŒHï¼ŒWï¼‰
}

 /**
 * @brief æ³•å‘ä¸€è‡´æ€§æŸå¤±
 * L_normal = mean((1-U) * (1 - <N_render, N_prior>))
 */
inline torch::Tensor normal_consistency_loss(
    torch::Tensor& rendered_depth,      //æ¸²æŸ“æ·±åº¦ï¼ˆHï¼ŒWï¼‰
    torch::Tensor& prior_depth,         //å…ˆéªŒæ·±åº¦ï¼ˆHï¼ŒWï¼‰
    torch::Tensor& uncertainty_map,     // ä¸ç¡®å®šæ€§å›¾ï¼ˆHï¼ŒWï¼‰
    float fx,float fy
)
{
    // è®¡ç®—ä¸¤ä¸ªæ³•å‘å›¾
    auto normal_render = depth_to_normal(rendered_depth,fx,fy); // (3, H, W)
    auto normal_prior = depth_to_normal(prior_depth,fx,fy); // (3, H, W)

    // ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆé€åƒç´ ç‚¹ç§¯ï¼‰
    auto consine_sim = (normal_render * normal_prior).sum(0); //(H,W)

    // æ³•å‘è¯¯å·®
    auto normal_error = 1.0f -consine_sim;

    // ç½®ä¿¡åº¦åŠ æƒï¼Œä¸ç¡®å®šæ€§ä½çš„åŒºåŸŸæƒé‡é«˜
    auto confidence = 1.0f - uncertainty_map;
    // auto confidence = torch::clamp(1.0f - uncertainty_map, 0.0f, 1.0f);
    auto weighted_error = normal_error * confidence;

    // åªè®¡ç®—æœ‰æ•ˆåŒºåŸŸ
    auto valid_mask = (prior_depth > 0.1f) & (rendered_depth > 0.1f);
    auto masked_error = weighted_error * valid_mask.to(torch::kFloat32);

    auto num_valid = valid_mask.sum() + 1e-6f;

    return masked_error.sum()/num_valid;
}

/**
 * @brief å®Œæ•´çš„å‡ ä½•çº¦æŸæŸå¤±
 */

inline torch::Tensor geometric_constraint_loss(
    torch::Tensor& rendered_depth,
    torch::Tensor& prior_depth,
    torch::Tensor& uncertainty_map,
    float fx,float fy,
    float lambda_depth = 0.1f,
    float lambda_normal = 0.05f
)
{
    auto L_depth = uncertainty_weighted_depth_loss(
        rendered_depth,prior_depth,uncertainty_map);

    auto L_normal = normal_consistency_loss(
        rendered_depth,prior_depth,uncertainty_map,fx,fy);

    return lambda_depth * L_depth + lambda_normal * L_normal;
    
}

inline torch::Tensor pgc_loss(
    torch::Tensor& rendered_depth,          //æ¸²æŸ“æ·±åº¦
    torch::Tensor& refined_depth,           //è¡¥å…¨æ·±åº¦
    torch::Tensor& uncertainty_map,         //ä¸ç¡®å®šæ€§å›¾
    float threshold = 0.8f,
    float epsilon = 1e-6f
)
{
    // æƒé‡å‡½æ•°ï¼šÏ‰(u) = (1/Ïƒ) * ğŸ™(Ïƒ < Ï„)
    auto weight = torch::where(
        uncertainty_map < threshold,
        1.0f / (uncertainty_map + epsilon),
        torch::zeros_like(uncertainty_map)
    );

    // åŠ æƒ L1 æŸå¤±
    auto diff = torch::abs(rendered_depth - refined_depth);
    return (weight * diff).sum() / weight.sum().clamp_min(1e-6f);
}


// ================Probabilistic Gaussian-LIC===================== â†‘

// ================== æ··åˆæ·±åº¦æŸå¤± - åˆ†è€Œæ²»ä¹‹ ==================
// ç›´æ¥å¤åˆ¶ä»¥ä¸‹ä»£ç æ›¿æ¢ loss_utils.h ç¬¬ 359-429 è¡Œ

/**
 * @brief åœ¨çº¿å°ºåº¦å¯¹é½ - æ‰‹åŠ¨æœ€å°äºŒä¹˜æ‹Ÿåˆ
 * é¿å…ä½¿ç”¨ torch::linalg::lstsq ä»¥è§£å†³ LibTorch API å…¼å®¹æ€§é—®é¢˜
 */
inline std::pair<float, float> compute_scale_alignment(
    const torch::Tensor& target_depth,
    const torch::Tensor& pred_depth,
    const torch::Tensor& valid_mask
) {
    auto target_d = target_depth.masked_select(valid_mask);
    auto pred_d = pred_depth.masked_select(valid_mask);
    
    int n = target_d.size(0);
    if (n < 10) {
        return {1.0f, 0.0f};
    }
    
    // æ‰‹åŠ¨è®¡ç®—æœ€å°äºŒä¹˜ï¼šs * pred + t = target
    auto mean_target = target_d.mean();
    auto mean_pred = pred_d.mean();
    
    auto pred_centered = pred_d - mean_pred;
    auto target_centered = target_d - mean_target;
    
    // scale = Î£((target - mean_t) * (pred - mean_p)) / Î£((pred - mean_p)^2)
    auto numerator = (target_centered * pred_centered).sum();
    auto denominator = (pred_centered * pred_centered).sum();
    
    float denom_val = denominator.item<float>();
    if (std::abs(denom_val) < 1e-8f) {
        return {1.0f, 0.0f};
    }
    
    float scale = numerator.item<float>() / denom_val;
    float shift = mean_target.item<float>() - scale * mean_pred.item<float>();
    
    // ç‰©ç†çº¦æŸ
    scale = std::clamp(scale, 0.1f, 10.0f);
    
    return {scale, shift};
}

/**
 * @brief æ··åˆæ·±åº¦æŸå¤± - åˆ†è€Œæ²»ä¹‹ç­–ç•¥
 */
inline torch::Tensor hybrid_depth_loss(
    torch::Tensor& render_depth,
    torch::Tensor& sgsnet_depth,
    torch::Tensor& sgsnet_uncertainty,
    torch::Tensor& sparse_depth,
    float lambda_lidar = 1.0f,
    float lambda_sgs = 0.1f,
    bool use_scale_align = true,
    bool print_debug = false
) {
    auto valid_mask = (sparse_depth > 0.1f).detach();
    int num_lidar_pts = valid_mask.sum().item<int>();
    
    float scale = 1.0f, shift = 0.0f;
    torch::Tensor aligned_sgs_depth;
    
    if (use_scale_align && num_lidar_pts > 10) {
        std::tie(scale, shift) = compute_scale_alignment(
            sparse_depth, sgsnet_depth, valid_mask
        );
        aligned_sgs_depth = (sgsnet_depth * scale + shift).detach();
    } else {
        aligned_sgs_depth = sgsnet_depth.detach();
    }
    
    // LiDAR å¼ºçº¦æŸ
    auto loss_lidar = torch::abs(render_depth - sparse_depth) * 
                      valid_mask.to(torch::kFloat32);
    
    // SGSNet è½¯çº¦æŸ
    auto sgs_weight = (1.0f - sgsnet_uncertainty).clamp(0.0f, 1.0f).detach();
    auto hole_mask = (~valid_mask).to(torch::kFloat32);
    auto loss_sgs = torch::abs(render_depth - aligned_sgs_depth) * 
                    hole_mask * sgs_weight;
    
    float total_pixels = static_cast<float>(render_depth.numel()) + 1e-6f;
    auto lidar_contrib = loss_lidar.sum() * lambda_lidar / total_pixels;
    auto sgs_contrib = loss_sgs.sum() * lambda_sgs / total_pixels;
    auto loss = lidar_contrib + sgs_contrib;
    
    if (print_debug) {
        std::cout << "[Hybrid Depth] "
                  << "LiDAR=" << num_lidar_pts 
                  << " Scale=" << scale 
                  << " | L_lidar=" << lidar_contrib.item<float>()
                  << " L_sgs=" << sgs_contrib.item<float>()
                  << " Total=" << loss.item<float>() << std::endl;
    }
    
    return loss;
}

/**
 * @brief å®Œæ•´çš„æ··åˆå‡ ä½•çº¦æŸæŸå¤±
 */
inline torch::Tensor hybrid_geometric_constraint_loss(
    torch::Tensor& render_depth,
    torch::Tensor& sgsnet_depth,
    torch::Tensor& sgsnet_uncertainty,
    torch::Tensor& sparse_depth,
    float fx, float fy,
    float lambda_lidar = 1.0f,
    float lambda_sgs = 0.1f,
    float lambda_normal = 0.05f,
    bool use_scale_align = true
) {
    auto L_depth = hybrid_depth_loss(
        render_depth, sgsnet_depth, sgsnet_uncertainty, sparse_depth,
        lambda_lidar, lambda_sgs, use_scale_align, false
    );
    
    auto valid_mask = (sparse_depth > 0.1f).detach();
    float scale = 1.0f, shift = 0.0f;
    if (use_scale_align) {
        std::tie(scale, shift) = compute_scale_alignment(
            sparse_depth, sgsnet_depth, valid_mask
        );
    }
    auto aligned_sgs = (sgsnet_depth * scale + shift).detach();
    
    auto L_normal = normal_consistency_loss(
        render_depth, aligned_sgs, sgsnet_uncertainty, fx, fy
    );
    
    return L_depth + lambda_normal * L_normal;
}

// ================== æ··åˆæ·±åº¦æŸå¤± - ç»“æŸ ==================

}
